{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.57.1","metadata":{"execution":{"iopub.status.busy":"2025-11-24T15:45:49.689949Z","iopub.execute_input":"2025-11-24T15:45:49.690196Z","iopub.status.idle":"2025-11-24T15:46:04.28332Z","shell.execute_reply.started":"2025-11-24T15:45:49.690175Z","shell.execute_reply":"2025-11-24T15:46:04.282356Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shap\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel, PreTrainedModel, PretrainedConfig, BertTokenizer\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\ndevice = torch.device('cuda')\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2025-11-24T15:47:18.022933Z","iopub.execute_input":"2025-11-24T15:47:18.023671Z","iopub.status.idle":"2025-11-24T15:47:18.758572Z","shell.execute_reply.started":"2025-11-24T15:47:18.023636Z","shell.execute_reply":"2025-11-24T15:47:18.757936Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BERTConfig(PretrainedConfig):\n    model_type = \"bert_with_absa\"\n\n    def __init__(self, absa_method=None, num_classes=2, class_weight=None, **kwargs):\n        super().__init__(**kwargs)\n        self.absa_method = absa_method\n        self.num_classes = num_classes\n        self.class_weight = class_weight\n\nclass InnerBert(nn.Module):\n    def __init__(self, num_classes):\n        super(InnerBert, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(768, num_classes)\n        \n\n    def forward(self, inputs_embeds=None, attention_mask=None, token_type_ids=None):      \n        _, pooled_output = self.bert(inputs_embeds=inputs_embeds, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=False)\n        x = self.dropout(pooled_output)\n        logits = self.fc(x)\n\n        return logits\n\nclass BERTModel(PreTrainedModel):\n    config_class = BERTConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_classes = config.num_classes\n        self.absa_method = config.absa_method\n        \n        self.bert_sent = InnerBert(self.num_classes)\n\n        if self.absa_method:\n            self.absa_fc = nn.Linear(1, 768)\n            self.bert_absa = InnerBert(self.num_classes)\n\n        if config.class_weight is not None:\n            class_weight = torch.tensor(config.class_weight, dtype=torch.float32)\n            self.criterion = nn.CrossEntropyLoss(weight=class_weight.to(device))\n        else:\n            self.criterion = nn.CrossEntropyLoss()\n\n        self.init_weights()\n\n\n    def forward(self, input_ids=None, absa_1=None, absa_2=None, attention_mask=None, token_type_ids=None, labels=None):\n        inputs_embeds = self.bert_sent.bert.embeddings(input_ids=input_ids, token_type_ids=token_type_ids)\n        logits_sent = self.bert_sent(inputs_embeds, attention_mask, token_type_ids)\n        \n        if self.absa_method:\n            absa_1 = self.absa_fc(absa_1)\n            absa_2 = self.absa_fc(absa_2)\n            absa_concat = torch.cat((absa_1, absa_2), dim=1)\n            token_type_ids_absa = torch.tensor([0, 1]).unsqueeze(0).repeat(absa_concat.shape[0], 1).to(\"cuda\")\n            \n            logits_absa = self.bert_absa(absa_concat, None, token_type_ids_absa)\n            \n            logits_sent += logits_absa\n        \n        loss = None\n        if labels is not None:\n            loss = self.criterion(logits_sent, labels)\n    \n        return SequenceClassifierOutput(loss=loss, logits=logits_sent)","metadata":{"execution":{"iopub.status.busy":"2025-11-24T15:47:23.914354Z","iopub.execute_input":"2025-11-24T15:47:23.914646Z","iopub.status.idle":"2025-11-24T15:47:23.926597Z","shell.execute_reply.started":"2025-11-24T15:47:23.914624Z","shell.execute_reply":"2025-11-24T15:47:23.925713Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_inference_model(model, s1, s2, absa_1, absa_2):\n    tokens_a = bert_tokenizer.tokenize(s1)\n    tokens_b = bert_tokenizer.tokenize(s2)\n\n    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n    segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n    print(\"len_token_s1\", len(tokens_a))\n    print(\"len_token_s2\", len(tokens_b))\n    \n    input_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n    input_mask = [1] * len(input_ids)\n    padding = [0] * (128 - len(input_ids))\n\n    input_ids += padding\n    input_mask += padding\n    segment_ids += padding\n\n    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n    attention_mask = torch.tensor([input_mask], dtype=torch.long).to(device)\n    segment_ids = torch.tensor([segment_ids], dtype=torch.long).to(device)\n    \n    if absa_1 and absa_2:\n        absa_1 = torch.tensor([absa_1], dtype=torch.float32).unsqueeze(1).unsqueeze(1).to(device)\n        absa_2 = torch.tensor([absa_2], dtype=torch.float32).unsqueeze(1).unsqueeze(1).to(device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, absa_1=absa_1, absa_2=absa_2, attention_mask=attention_mask, token_type_ids=segment_ids).logits\n        softmax = torch.nn.Softmax(dim=-1)\n        probs = softmax(outputs)\n        return probs","metadata":{"execution":{"iopub.status.busy":"2025-11-24T15:47:24.578824Z","iopub.execute_input":"2025-11-24T15:47:24.579554Z","iopub.status.idle":"2025-11-24T15:47:24.588831Z","shell.execute_reply.started":"2025-11-24T15:47:24.579519Z","shell.execute_reply":"2025-11-24T15:47:24.588031Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_shap_values(model, s1, s2, absa_1, absa_2, len_token_s1, len_token_s2):   \n    def f(x):\n        \"\"\"Function to explain - takes token strings\"\"\"\n        absa_1_t = torch.tensor([absa_1], dtype=torch.float32).unsqueeze(1).unsqueeze(1).to(device)\n        absa_2_t = torch.tensor([absa_2], dtype=torch.float32).unsqueeze(1).unsqueeze(1).to(device)\n        \n        segment_ids = [0] * (len_token_s1 + 2) + [1] * (len_token_s2 + 1)\n        \n        tv = [bert_tokenizer.tokenize(v) for v in x]\n        for i in range(len(tv)):\n            tv[i] = ['[CLS]'] + tv[i] + ['[SEP]']\n        input_ids_list = []\n        attention_mask_list = []\n        segment_ids_list = []\n        absa_1_list = []\n        absa_2_list = []\n\n        for tokens in tv:\n            new_segment_ids = segment_ids\n            input_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n            \n            input_mask = [1] * len(input_ids)\n            padding = [0] * (128 - len(input_ids))\n            \n            input_ids += padding\n            input_mask += padding\n            new_segment_ids = segment_ids + [0] * (128 - len(new_segment_ids))\n            \n            input_ids_list.append(torch.tensor([input_ids], dtype=torch.long).to(device))\n            attention_mask_list.append(torch.tensor([input_mask], dtype=torch.long).to(device))\n            segment_ids_list.append(torch.tensor([new_segment_ids], dtype=torch.long).to(device))\n            if absa_1 and absa_2:\n                absa_1_list.append(absa_1_t)\n                absa_2_list.append(absa_2_t)\n        \n        input_ids = torch.cat(input_ids_list, dim=0)\n        attention_mask = torch.cat(attention_mask_list, dim=0)\n        segment_ids = torch.cat(segment_ids_list, dim=0)\n        if absa_1_t and absa_2_t:\n            absa_1_t = torch.cat(absa_1_list, dim=0)\n            absa_2_t = torch.cat(absa_2_list, dim=0)\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, absa_1=absa_1_t, absa_2=absa_2_t, attention_mask=attention_mask, token_type_ids=segment_ids).logits\n        return outputs.cpu().numpy()\n    \n    # Create explainer\n    explainer = shap.Explainer(f, bert_tokenizer, output_names=[\"-1\", \"0\", \"1\"])\n\n    test = [f\"{s1} [SEP] {s2}\"]\n\n    shap_values = explainer(test)\n    \n    return shap_values\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T15:47:24.802651Z","iopub.execute_input":"2025-11-24T15:47:24.803355Z","iopub.status.idle":"2025-11-24T15:47:24.812681Z","shell.execute_reply.started":"2025-11-24T15:47:24.803326Z","shell.execute_reply":"2025-11-24T15:47:24.811764Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load dataset and prepare\nfrom datasets import load_dataset\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n\n# Load dataset\nprint(\"Loading dataset from Hugging Face...\")\ndataset = load_dataset(\"trungpq/rlcc-new-data-appearance\")\nprint(f\"Dataset loaded. Total samples: {len(dataset['test'])}\")\n\n# Load ALL samples - but will filter out invalid ones\nraw_data = []\nfor sample in dataset['test']:\n    s1 = sample.get('sentences_1')\n    s2 = sample.get('sentences_2')\n    appearance = sample.get('appearance')\n    \n    raw_data.append({\n        's1': s1,\n        's2': s2,\n        'absa_1': sample.get('absa_min_1', 0),\n        'absa_2': sample.get('absa_min_2', 0),\n        'label': appearance\n    })\n\nprint(f\"\\nTotal samples in dataset: {len(raw_data)}\")\n\n# Filter: keep only samples with valid labels and both s1 and s2 present\nfiltered_data = []\nfor sample in raw_data:\n    # Check if both s1 and s2 exist\n    if not sample['s1'] or not sample['s2']:\n        continue\n    \n    # Check if label is valid (-1, 0, or 1)\n    if sample['label'] not in [-1, 0, 1]:\n        continue\n    \n    # Normalize s1 and s2\n    s1_normalized = sample['s1'].lower().replace(\",\", \"\").replace(\".\", \"\")\n    s2_normalized = sample['s2'].lower().replace(\",\", \"\").replace(\".\", \"\")\n    \n    filtered_data.append({\n        's1': s1_normalized,\n        's2': s2_normalized,\n        'absa_1': sample['absa_1'],\n        'absa_2': sample['absa_2'],\n        'label': sample['label']\n    })\n\nprint(f\"Samples after filtering (valid labels + both s1 and s2): {len(filtered_data)}\")\n\n# Show some sample statistics\nif filtered_data:\n    print(f\"\\nSample data (first 3):\")\n    for i, sample in enumerate(filtered_data[:3]):\n        print(f\"\\nSample {i+1}:\")\n        print(f\"  Label: {sample['label']}\")\n        print(f\"  S1: {sample['s1'][:80]}...\")\n        print(f\"  S2: {sample['s2'][:80]}...\")\n        print(f\"  ABSA1: {sample['absa_1']}, ABSA2: {sample['absa_2']}\")\n\n# Load model\nprint(\"\\n\" + \"=\"*80)\nprint(\"Loading appearance model...\")\nappearance_model = BERTModel.from_pretrained(\"trungpq/rlcc-new-appearance-upsample_replacement-absa-min\").to(device)\nappearance_model = appearance_model.eval()\nprint(\"✓ Model loaded successfully\")\n\n# filtered_data = filtered_data[:30]\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T15:47:25.005619Z","iopub.execute_input":"2025-11-24T15:47:25.005962Z","iopub.status.idle":"2025-11-24T15:47:34.561158Z","shell.execute_reply.started":"2025-11-24T15:47:25.005936Z","shell.execute_reply":"2025-11-24T15:47:34.560319Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# PART 1: Baseline Evaluation - Get initial F1 score on full dataset\n\ndef get_model_predictions(model, s1, s2, absa_1, absa_2):\n    \"\"\"Get model predictions for a single sample\"\"\"\n    tokens_a = bert_tokenizer.tokenize(s1)\n    tokens_b = bert_tokenizer.tokenize(s2)\n\n    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n    segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n    \n    input_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n    input_mask = [1] * len(input_ids)\n    padding = [0] * (128 - len(input_ids))\n\n    input_ids += padding\n    input_mask += padding\n    segment_ids += padding\n\n    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n    attention_mask = torch.tensor([input_mask], dtype=torch.long).to(device)\n    segment_ids = torch.tensor([segment_ids], dtype=torch.long).to(device)\n    \n    if absa_1 and absa_2:\n        absa_1 = torch.tensor([absa_1], dtype=torch.float32).unsqueeze(1).unsqueeze(1).to(device)\n        absa_2 = torch.tensor([absa_2], dtype=torch.float32).unsqueeze(1).unsqueeze(1).to(device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, absa_1=absa_1, absa_2=absa_2, \n                       attention_mask=attention_mask, token_type_ids=segment_ids).logits\n        softmax = torch.nn.Softmax(dim=-1)\n        probs = softmax(outputs)\n        pred_class = torch.argmax(probs, dim=1)\n    \n    return pred_class.item(), probs[0].cpu().numpy()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PART 1: BASELINE EVALUATION ON FULL DATASET\")\nprint(\"=\"*80)\n\n# Get predictions on all samples\nall_preds = []\nall_labels = []\nlabel_mapping = {-1: 0, 0: 1, 1: 2}  # Map labels to indices\ninverse_mapping = {0: -1, 1: 0, 2: 1}\n\nprint(f\"\\nEvaluating {len(filtered_data)} samples...\")\nfor idx, sample in enumerate(filtered_data):\n    if idx % max(1, len(filtered_data)//10) == 0:\n        print(f\"  Progress: {idx}/{len(filtered_data)}\")\n    \n    # All samples in filtered_data now have valid s1, s2, and labels\n    pred_class, probs = get_model_predictions(\n        appearance_model, \n        sample['s1'], sample['s2'],\n        sample['absa_1'], sample['absa_2']\n    )\n    all_preds.append(pred_class)\n    all_labels.append(sample['label'])  # Keep actual label from dataset\n\n# Calculate F1 for each class (binary: class_i vs others)\nprint(f\"\\n✓ Computing F1 scores for all 3 classes...\")\n\nf1_class_minus1 = f1_score(\n    [1 if l == -1 else 0 for l in all_labels],\n    [1 if p == label_mapping[-1] else 0 for p in all_preds],\n    zero_division=0\n)\n\nf1_class_0 = f1_score(\n    [1 if l == 0 else 0 for l in all_labels],\n    [1 if p == label_mapping[0] else 0 for p in all_preds],\n    zero_division=0\n)\n\nf1_class_1 = f1_score(\n    [1 if l == 1 else 0 for l in all_labels],\n    [1 if p == label_mapping[1] else 0 for p in all_preds],\n    zero_division=0\n)\n\nbaseline_macro_f1 = (f1_class_minus1 + f1_class_0 + f1_class_1) / 3.0\n\nprint(f\"\\n✓ Baseline Metrics (F1 per class - binary: class_i vs others):\")\nprint(f\"  F1 Score Class -1: {f1_class_minus1:.4f}\")\nprint(f\"  F1 Score Class 0:  {f1_class_0:.4f}\")\nprint(f\"  F1 Score Class 1:  {f1_class_1:.4f}\")\nprint(f\"  Macro F1:          {baseline_macro_f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T15:47:34.562584Z","iopub.execute_input":"2025-11-24T15:47:34.563625Z","iopub.status.idle":"2025-11-24T15:47:39.942618Z","shell.execute_reply.started":"2025-11-24T15:47:34.563597Z","shell.execute_reply":"2025-11-24T15:47:39.941933Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 2-5: SHAP Faithfulness Evaluation\n\nCompute SHAP values for all 3 classes, extract token importance from both S1 and S2,\nremove top-k and bottom-k tokens, and evaluate macro F1 scores.\n","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport nltk\nfrom tqdm import tqdm\n\nnltk.download('averaged_perceptron_tagger_eng')\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PART 2: COMPUTE SHAP VALUES AND BUILD IMPORTANT TOKEN SETS\")\nprint(\"=\"*80)\n\n# Build aggregated SHAP values across samples\nall_token_shap_values = defaultdict(lambda: {'positive': [], 'negative': []})\nsample_token_data = []\n\nprint(f\"\\nComputing SHAP values for {len(filtered_data)} samples...\")\n\nfor sample_idx, sample in enumerate(tqdm(filtered_data, desc=\"Processing\")):\n    s1 = sample['s1']\n    s2 = sample['s2']\n    absa_1 = sample['absa_1']\n    absa_2 = sample['absa_2']\n    \n    # Skip if s1 or s2 is null\n    if s1 is None or s2 is None:\n        continue\n    \n    # Tokenize\n    tokens_a = bert_tokenizer.tokenize(s1)\n    tokens_b = bert_tokenizer.tokenize(s2)\n    len_token_s1 = len(tokens_a)\n    len_token_s2 = len(tokens_b)\n    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n    \n    # Create SHAP explanation function for this sample\n    def f_shap(x):\n        \"\"\"Function to explain - takes token strings\"\"\"\n        absa_1_t = torch.tensor([absa_1], dtype=torch.float32).unsqueeze(1).unsqueeze(1).to(device)\n        absa_2_t = torch.tensor([absa_2], dtype=torch.float32).unsqueeze(1).unsqueeze(1).to(device)\n        \n        segment_ids_base = [0] * (len_token_s1 + 2) + [1] * (len_token_s2 + 1)\n        \n        tv = [bert_tokenizer.tokenize(v) for v in x]\n        input_ids_list = []\n        attention_mask_list = []\n        segment_ids_list = []\n\n        for tokens_var in tv:\n            tokens_var = ['[CLS]'] + tokens_var + ['[SEP]']\n            input_ids = bert_tokenizer.convert_tokens_to_ids(tokens_var)\n            \n            input_mask = [1] * len(input_ids)\n            padding = [0] * (128 - len(input_ids))\n            \n            input_ids += padding\n            input_mask += padding\n            segment_ids_padded = segment_ids_base + [0] * (128 - len(segment_ids_base))\n            \n            input_ids_list.append(torch.tensor([input_ids], dtype=torch.long).to(device))\n            attention_mask_list.append(torch.tensor([input_mask], dtype=torch.long).to(device))\n            segment_ids_list.append(torch.tensor([segment_ids_padded], dtype=torch.long).to(device))\n        \n        input_ids = torch.cat(input_ids_list, dim=0)\n        attention_mask = torch.cat(attention_mask_list, dim=0)\n        segment_ids = torch.cat(segment_ids_list, dim=0)\n        \n        with torch.no_grad():\n            outputs = appearance_model(input_ids=input_ids, absa_1=absa_1_t, absa_2=absa_2_t, \n                                      attention_mask=attention_mask, token_type_ids=segment_ids).logits\n        return outputs.cpu().numpy()\n    \n    # Get SHAP values\n    try:\n        explainer = shap.Explainer(f_shap, bert_tokenizer, output_names=[\"class_-1\", \"class_0\", \"class_1\"])\n        test_input = [f\"{s1} [SEP] {s2}\"]\n        shap_values = explainer(test_input, silent=True)\n        \n        # Extract SHAP values for all 3 classes\n        # shap_values.values shape: (1, num_tokens, 3)\n        shap_vals_class_minus1 = shap_values.values[0, :, 0]  # Class -1\n        shap_vals_class_0 = shap_values.values[0, :, 1]  # Class 0\n        shap_vals_class_1 = shap_values.values[0, :, 2]  # Class 1\n        \n        # Store sample token data\n        sample_token_data.append({\n            'sample_idx': sample_idx,\n            's1_tokens': tokens_a,\n            's2_tokens': tokens_b,\n            'shap_values_minus1': shap_vals_class_minus1,\n            'shap_values_0': shap_vals_class_0,\n            'shap_values_1': shap_vals_class_1,\n            'tokens': tokens,\n            's1': s1,\n            's2': s2,\n            'absa_1': absa_1,\n            'absa_2': absa_2,\n            'label': sample['label']\n        })\n        \n    except Exception as e:\n        continue\n\nprint(f\"\\n✓ SHAP values computed for {len(sample_token_data)} samples\")","metadata":{"execution":{"iopub.status.busy":"2025-11-24T15:47:44.942888Z","iopub.execute_input":"2025-11-24T15:47:44.943196Z","iopub.status.idle":"2025-11-24T16:02:06.89659Z","shell.execute_reply.started":"2025-11-24T15:47:44.943173Z","shell.execute_reply":"2025-11-24T16:02:06.895793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# PART 3: Token Removal Test - Per-Sample Analysis\n\ndef merge_wordpieces(tokens):\n    \"\"\"Merge ##word tokens với token trước đó\"\"\"\n    merged_tokens = []\n    merged_indices = []\n    \n    for i, tok in enumerate(tokens):\n        if tok.startswith(\"##\"):\n            merged_tokens[-1] += tok[2:]\n            merged_indices[-1].append(i)\n        else:\n            merged_tokens.append(tok)\n            merged_indices.append([i])\n    return merged_tokens, merged_indices\n\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PART 3: PREPARE FOR PER-SAMPLE TOKEN REMOVAL\")\nprint(\"=\"*80)\n\n# Build token importance per sample (using absolute SHAP values, for all 3 classes)\n# Average SHAP importance across all 3 classes\nsample_token_importance = []\n\nfor data in sample_token_data:\n    s1 = data['s1']\n    s2 = data['s2']\n    s1_tokens = data['s1_tokens']\n    s2_tokens = data['s2_tokens']\n    shap_values_minus1 = data['shap_values_minus1']\n    shap_values_0 = data['shap_values_0']\n    shap_values_1 = data['shap_values_1']\n    label = data.get('label', -1)\n    \n    if s1 is None or s2 is None:\n        continue\n    \n    # Get indices for S1 and S2 tokens\n    s1_start_idx = 1  # After [CLS]\n    s1_end_idx = 1 + len(s1_tokens)  # Before first [SEP]\n    s2_start_idx = s1_end_idx + 1  # After first [SEP]\n    s2_end_idx = s2_start_idx + len(s2_tokens)  # Before second [SEP]\n    \n    # Extract SHAP for S1 and S2 from each class\n    s1_shap_minus1 = shap_values_minus1[s1_start_idx:s1_end_idx]\n    s1_shap_0 = shap_values_0[s1_start_idx:s1_end_idx]\n    s1_shap_1 = shap_values_1[s1_start_idx:s1_end_idx]\n    \n    s2_shap_minus1 = shap_values_minus1[s2_start_idx:s2_end_idx]\n    s2_shap_0 = shap_values_0[s2_start_idx:s2_end_idx]\n    s2_shap_1 = shap_values_1[s2_start_idx:s2_end_idx]\n    \n    # Average SHAP values across 3 classes\n    s1_shap_avg = (np.abs(s1_shap_minus1) + np.abs(s1_shap_0) + np.abs(s1_shap_1)) / 3.0\n    s2_shap_avg = (np.abs(s2_shap_minus1) + np.abs(s2_shap_0) + np.abs(s2_shap_1)) / 3.0\n    \n    # Merge wordpieces for S1 and S2\n    merged_tokens_s1, merged_indices_s1 = merge_wordpieces(s1_tokens)\n    merged_tokens_s2, merged_indices_s2 = merge_wordpieces(s2_tokens)\n    \n    # Combine S1 and S2 tokens for overall importance ranking\n    all_tokens = merged_tokens_s1 + merged_tokens_s2\n    \n    # Create importance mapping for all tokens\n    merged_importance_all = {}\n    \n    # S1 tokens (indices 0 to len(merged_tokens_s1)-1)\n    for merged_idx, original_indices in enumerate(merged_indices_s1):\n        total_importance = sum(s1_shap_avg[orig_idx] for orig_idx in original_indices \n                               if orig_idx < len(s1_shap_avg))\n        merged_importance_all[merged_idx] = total_importance\n    \n    # S2 tokens (indices len(merged_tokens_s1) to end)\n    for merged_idx, original_indices in enumerate(merged_indices_s2):\n        s1_len = len(merged_tokens_s1)\n        total_importance = sum(s2_shap_avg[orig_idx] for orig_idx in original_indices \n                               if orig_idx < len(s2_shap_avg))\n        merged_importance_all[s1_len + merged_idx] = total_importance\n    \n    # Sort all tokens by importance\n    sorted_by_importance = sorted(enumerate(all_tokens), \n                                  key=lambda x: merged_importance_all.get(x[0], 0), \n                                  reverse=True)\n    \n    sample_token_importance.append({\n        'sample_idx': data['sample_idx'],\n        's1': s1,\n        's2': s2,\n        'absa_1': data['absa_1'],\n        'absa_2': data['absa_2'],\n        'label': label,\n        'all_tokens': all_tokens,  # Combined S1 + S2 tokens\n        's1_len': len(merged_tokens_s1),\n        's2_len': len(merged_tokens_s2),\n        'sorted_by_importance': sorted_by_importance  # [(idx, token), ...]\n    })\n\nprint(f\"\\n✓ Prepared {len(sample_token_importance)} samples for token removal analysis\")\nprint(f\"  Each sample will have independent top-k and bot-k tokens\")\nprint(f\"  Token importance averaged across all 3 classes (-1, 0, 1)\")\nprint(f\"  Tokens from both S1 and S2 are ranked together\")\n\n# Count adjectives in each sample for k_max determination\nimport nltk\ntry:\n    nltk.data.find('taggers/averaged_perceptron_tagger')\nexcept LookupError:\n    nltk.download('averaged_perceptron_tagger')\n\nmax_adj_count = 0\nfor sample_info in sample_token_importance:\n    all_tokens = sample_info['all_tokens']\n    # POS tagging to identify adjectives\n    pos_tags = nltk.pos_tag(all_tokens)\n    adj_count = sum(1 for word, tag in pos_tags if tag.startswith('JJ'))  # JJ, JJR, JJS = adjectives\n    max_adj_count = max(max_adj_count, adj_count)\n\nprint(f\"\\n✓ Max number of adjectives found: {max_adj_count}\")\nprint(f\"  This will be used as k_max for token removal experiments\")","metadata":{"execution":{"iopub.status.busy":"2025-11-24T16:03:45.551761Z","iopub.execute_input":"2025-11-24T16:03:45.551951Z","iopub.status.idle":"2025-11-24T16:03:45.962857Z","shell.execute_reply.started":"2025-11-24T16:03:45.551935Z","shell.execute_reply":"2025-11-24T16:03:45.962186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PART 4: Token Removal and Macro F1 Evaluation\n\ndef remove_tokens_from_s1_and_s2(s1_text, s2_text, tokens_to_remove):\n    \"\"\"Remove specified tokens from both s1 and s2\"\"\"\n    s1_result = None\n    s2_result = None\n    \n    if s1_text is not None:\n        s1_tokens = bert_tokenizer.tokenize(s1_text)\n        merged_tokens_s1, _ = merge_wordpieces(s1_tokens)\n        remaining_tokens_s1 = [t for t in merged_tokens_s1 if t not in tokens_to_remove]\n        s1_result = \" \".join(remaining_tokens_s1) if remaining_tokens_s1 else \"\"\n    \n    if s2_text is not None:\n        s2_tokens = bert_tokenizer.tokenize(s2_text)\n        merged_tokens_s2, _ = merge_wordpieces(s2_tokens)\n        remaining_tokens_s2 = [t for t in merged_tokens_s2 if t not in tokens_to_remove]\n        s2_result = \" \".join(remaining_tokens_s2) if remaining_tokens_s2 else \"\"\n    \n    return s1_result, s2_result\n\n\ndef evaluate_macro_f1_after_removal(model, data, removed_tokens):\n    \"\"\"Evaluate macro F1 across all 3 classes after removing tokens from both s1 and s2\"\"\"\n    predictions = []\n    labels = []\n    \n    for sample in data:\n        # If s1 or s2 is null, default to NOT -1\n        if sample['s1'] is None or sample['s2'] is None:\n            # Default: not class -1, use class 0 (index 1)\n            pred_class = label_mapping[0]  # index 1\n            predictions.append(0)\n        else:\n            # Remove tokens from both s1 and s2\n            if removed_tokens:\n                s1_modified, s2_modified = remove_tokens_from_s1_and_s2(\n                    sample['s1'], sample['s2'], removed_tokens\n                )\n            else:\n                s1_modified = sample['s1']\n                s2_modified = sample['s2']\n            \n            # Handle empty s1 or s2 after removal\n            if not s1_modified or not s2_modified:\n                pred_class = label_mapping[0]  # Default to class 0 if either is empty\n            else:\n                # Use modified s1 and s2\n                pred_class, _ = get_model_predictions(\n                    model,\n                    s1_modified, s2_modified,  # Both modified\n                    sample['absa_1'], sample['absa_2']\n                )\n            \n            # Map back to original class labels\n            predictions.append(inverse_mapping[pred_class])\n        \n        # Ground truth\n        labels.append(sample['label'])\n    \n    # Calculate F1 for each class (binary: class_i vs others)\n    f1_class_minus1 = f1_score([1 if l == -1 else 0 for l in labels], \n                               [1 if p == -1 else 0 for p in predictions], \n                               zero_division=0)\n    f1_class_0 = f1_score([1 if l == 0 else 0 for l in labels], \n                          [1 if p == 0 else 0 for p in predictions], \n                          zero_division=0)\n    f1_class_1 = f1_score([1 if l == 1 else 0 for l in labels], \n                          [1 if p == 1 else 0 for p in predictions], \n                          zero_division=0)\n    \n    # Calculate macro F1\n    macro_f1 = (f1_class_minus1 + f1_class_0 + f1_class_1) / 3.0\n    \n    return macro_f1\n\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PART 4: PER-SAMPLE TOKEN REMOVAL - MACRO F1 EVALUATION\")\nprint(\"=\"*80)\nprint(\"(Remove top-k and bot-k ADJECTIVE tokens from S1 & S2 for each sample, evaluate macro F1)\\n\")\n\n# Use max adjective count as k_max, but cap at 10\n# k_max = min(10, max_adj_count)\nk_max = max_adj_count\nprint(f\"k_max = {k_max} (max {k_max} adjectives, or less if samples have fewer adjectives)\\n\")\n\nresults_macro_f1 = {\n    'k_values': [],\n    'top_k_f1': [],\n    'bottom_k_f1': [],\n}\n\nfor k in range(1, k_max + 1):\n    print(f\"Evaluating k={k}...\", end=\" \")\n    \n    # Collect ADJECTIVE tokens to remove for this k value\n    top_k_tokens_list = []\n    bot_k_tokens_list = []\n    \n    for sample_info in sample_token_importance:\n        all_tokens = sample_info['all_tokens']\n        sorted_tokens = sample_info['sorted_by_importance']  # [(idx, token), ...]\n        \n        # Filter only adjectives from sorted tokens\n        pos_tags = nltk.pos_tag(all_tokens)\n        adjective_set = {word for word, tag in pos_tags if tag.startswith('JJ')}\n        \n        # Get adjectives from sorted list (by importance)\n        sorted_adjectives = [(idx, token) for idx, token in sorted_tokens if token in adjective_set]\n        \n        # Top-k: most important adjective tokens\n        if len(sorted_adjectives) >= k:\n            top_k = [token for idx, token in sorted_adjectives[:k]]\n            top_k_tokens_list.append(top_k)\n        else:\n            # Use all available adjectives if less than k\n            top_k_tokens_list.append([token for idx, token in sorted_adjectives])\n        \n        # Bot-k: least important adjective tokens\n        if len(sorted_adjectives) >= k:\n            bot_k = [token for idx, token in sorted_adjectives[-k:]]\n            bot_k_tokens_list.append(bot_k)\n        else:\n            # If sample has fewer adjectives than k, also use all adjectives (same as top-k)\n            bot_k_tokens_list.append([token for idx, token in sorted_adjectives])\n    \n    # Evaluate with top-k removed\n    f1_top_k_list = []\n    for sample_idx, sample in enumerate(filtered_data):\n        if sample_idx < len(top_k_tokens_list):\n            tokens_to_remove = top_k_tokens_list[sample_idx]\n            \n            if sample['s1'] is None or sample['s2'] is None:\n                pred_class = label_mapping[0]\n                pred = 0\n            else:\n                if tokens_to_remove:\n                    s1_modified, s2_modified = remove_tokens_from_s1_and_s2(\n                        sample['s1'], sample['s2'], tokens_to_remove\n                    )\n                else:\n                    s1_modified = sample['s1']\n                    s2_modified = sample['s2']\n                \n                if not s1_modified or not s2_modified:\n                    pred_class = label_mapping[0]\n                else:\n                    pred_class, _ = get_model_predictions(\n                        appearance_model,\n                        s1_modified, s2_modified,\n                        sample['absa_1'], sample['absa_2']\n                    )\n                \n                pred = inverse_mapping[pred_class]\n            \n            f1_top_k_list.append((sample['label'], pred))\n    \n    # Calculate macro F1 for top-k\n    if f1_top_k_list:\n        labels_top = [item[0] for item in f1_top_k_list]\n        preds_top = [item[1] for item in f1_top_k_list]\n        \n        # F1 for each class\n        f1_top_minus1 = f1_score([1 if l == -1 else 0 for l in labels_top], \n                                 [1 if p == -1 else 0 for p in preds_top], zero_division=0)\n        f1_top_0 = f1_score([1 if l == 0 else 0 for l in labels_top], \n                            [1 if p == 0 else 0 for p in preds_top], zero_division=0)\n        f1_top_1 = f1_score([1 if l == 1 else 0 for l in labels_top], \n                            [1 if p == 1 else 0 for p in preds_top], zero_division=0)\n        macro_f1_top = (f1_top_minus1 + f1_top_0 + f1_top_1) / 3.0\n    else:\n        macro_f1_top = baseline_macro_f1\n    \n    # Evaluate with bot-k removed\n    f1_bot_k_list = []\n    for sample_idx, sample in enumerate(filtered_data):\n        if sample_idx < len(bot_k_tokens_list):\n            tokens_to_remove = bot_k_tokens_list[sample_idx]\n            \n            if sample['s1'] is None or sample['s2'] is None:\n                pred_class = label_mapping[0]\n                pred = 0\n            else:\n                if tokens_to_remove:\n                    s1_modified, s2_modified = remove_tokens_from_s1_and_s2(\n                        sample['s1'], sample['s2'], tokens_to_remove\n                    )\n                else:\n                    s1_modified = sample['s1']\n                    s2_modified = sample['s2']\n                \n                if not s1_modified or not s2_modified:\n                    pred_class = label_mapping[0]\n                else:\n                    pred_class, _ = get_model_predictions(\n                        appearance_model,\n                        s1_modified, s2_modified,\n                        sample['absa_1'], sample['absa_2']\n                    )\n                \n                pred = inverse_mapping[pred_class]\n            \n            f1_bot_k_list.append((sample['label'], pred))\n    \n    # Calculate macro F1 for bot-k\n    if f1_bot_k_list:\n        labels_bot = [item[0] for item in f1_bot_k_list]\n        preds_bot = [item[1] for item in f1_bot_k_list]\n        \n        # F1 for each class\n        f1_bot_minus1 = f1_score([1 if l == -1 else 0 for l in labels_bot], \n                                 [1 if p == -1 else 0 for p in preds_bot], zero_division=0)\n        f1_bot_0 = f1_score([1 if l == 0 else 0 for l in labels_bot], \n                            [1 if p == 0 else 0 for p in preds_bot], zero_division=0)\n        f1_bot_1 = f1_score([1 if l == 1 else 0 for l in labels_bot], \n                            [1 if p == 1 else 0 for p in preds_bot], zero_division=0)\n        macro_f1_bot = (f1_bot_minus1 + f1_bot_0 + f1_bot_1) / 3.0\n    else:\n        macro_f1_bot = baseline_macro_f1\n    \n    results_macro_f1['k_values'].append(k)\n    results_macro_f1['top_k_f1'].append(macro_f1_top)\n    results_macro_f1['bottom_k_f1'].append(macro_f1_bot)\n    \n    print(f\"Top-{k}: {macro_f1_top:.4f} (drop: {(baseline_macro_f1-macro_f1_top):+.4f}), \" + \n          f\"Bot-{k}: {macro_f1_bot:.4f} (drop: {(baseline_macro_f1-macro_f1_bot):+.4f})\")\n\nprint(\"\\n✓ Per-sample token removal evaluation complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:05:33.94994Z","iopub.execute_input":"2025-11-24T16:05:33.950298Z","iopub.status.idle":"2025-11-24T16:09:38.780561Z","shell.execute_reply.started":"2025-11-24T16:05:33.950273Z","shell.execute_reply":"2025-11-24T16:09:38.779758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# PART 5: Visualization\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PART 5: VISUALIZING SHAP FAITHFULNESS\")\nprint(\"=\"*80)\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot macro F1 scores\nk_vals = results_macro_f1['k_values']\ntop_f1 = results_macro_f1['top_k_f1']\nbot_f1 = results_macro_f1['bottom_k_f1']\n\n# Calculate F1 drop (impact)\ntop_f1_drop = [baseline_macro_f1 - f1 for f1 in top_f1]\nbot_f1_drop = [baseline_macro_f1 - f1 for f1 in bot_f1]\n\nax.axhline(y=0, color='gray', linestyle='--', linewidth=2, label='No Impact', alpha=0.7)\nax.plot(k_vals, top_f1_drop, marker='o', linewidth=2.5, markersize=9, \n        label='Remove Top-k (High Impact)', color='darkred')\nax.plot(k_vals, bot_f1_drop, marker='s', linewidth=2.5, markersize=9, \n        label='Remove Bottom-k (Low Impact)', color='steelblue')\n\nax.set_xlabel('Number of tokens removed from S1 & S2 (k)', fontsize=13)\nax.set_ylabel('Macro F1 Drop (Impact)', fontsize=13)\nax.set_title('SHAP Faithfulness: Token Removal Impact on Macro F1', fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='best')\nax.grid(True, alpha=0.3)\nax.set_xticks(k_vals)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n✓ Visualization complete!\")\nprint(f\"\\nBaseline Macro F1 Score: {baseline_macro_f1:.4f}\")\nprint(f\"Max F1 Drop (Top-k):    {max(top_f1_drop):.4f} at k={k_vals[top_f1_drop.index(max(top_f1_drop))]}\")\nprint(f\"Max F1 Drop (Bottom-k): {max(bot_f1_drop):.4f} at k={k_vals[bot_f1_drop.index(max(bot_f1_drop))]}\")\nprint(\"\\nNote: Each sample has independent top-k and bottom-k tokens\")\nprint(\"Token importance averaged across all 3 classes (-1, 0, 1)\")\nprint(\"Tokens removed from both S1 and S2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:09:43.730968Z","iopub.execute_input":"2025-11-24T16:09:43.731255Z","iopub.status.idle":"2025-11-24T16:09:44.074474Z","shell.execute_reply.started":"2025-11-24T16:09:43.731235Z","shell.execute_reply":"2025-11-24T16:09:44.073625Z"}},"outputs":[],"execution_count":null}]}